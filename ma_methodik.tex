\chapter{Methodik}

\section{Time Delay Stability}\label{TDS}

Das Verfahren der \acs{TDS} stellt eine bivariate, lineare Methode zur Untersuchung multivariater Biosignalaufzeichnungen und zur Bestimmung von Korrelationen zweier Biosignale innerhalb eines Netzwerks dar. Die mittels \acs{PSG} aufgezeichneten Signale stellen hierbei die einzelnen Knoten des Netzwerks dar und repräsentieren jeweils eigenständige physiologische Systeme, die innerhalb des Netzwerks interagieren. Auf diese Weise können Aussagen über die Netztopologie sowie die Verbindungsstärke jeweils zweier Systeme getroffen werden. Diese Merkmale werden in Abhängigkeit der jeweiligen Schlafstadien untersucht. Als Schlafstadien gelten der Wachzustand, \acs{REM}-Schlaf und Tiefschlaf (\acs{NREM}3 und \acs{NREM}4 zusammengefasst) gemäß \acs{AASM} sowie der Leichtschlaf in Form des Schlafstadiums \acs{NREM}2. Das Schlafstadium \acs{NREM}1 wird hierbei außer Acht gelassen, da dieses als Übergang zwischen Wachzustand und Leichtschlaf (\acs{NREM}2) gilt. \parencite{bashan_network_2012, iber_aasm_2007}\\

Die Bestimmung einer Korrelation zwischen zwei Systemen erfolgt auf Basis der Kreuzkorrelation, jedoch werden zur Berechnung der Korrelationsstärke nicht die Kreuzkorrelationskoeffizienten verwendet, sondern die Zeit, über welche die Werte der maximalen Koeffizienten konstant sind. Im Folgenden wird beschrieben, in welcher Weise die zu untersuchenden Signale vorverarbeitet werden müssen und wie die Korrelationen anhand der \acs{TDS}-Analyse anschließend ermittelt werden. Auf die Implementierung dieses Verfahrens wird in Kapitel \ref{Implementierung} eingegangen.

%TODO:
%Bartsch et al. verwendet anderes edge handling (anschauen)! aber hier TDS Methode von Dagmar verwenden, damit die Ergebnisse vergleichbar sind

\subsection{Datenvorverarbeitung}\label{datenvorverarbeitung}

Da die Biosignale zum einen sehr unterschiedliche Merkmale aufweisen (vgl. Abb. \ref{fig:beispiel-psg}), zum anderen mit uneinheitlichen Abtastraten aufgezeichnet werden können und darüber hinaus die Kreuzkorrelation maßgeblich von den Signalwerten (Amplituden) abhängt, muss zunächst eine Vergleichbarkeit der Daten herbeigeführt werden. Zu diesem Zweck werden Zeitreihen extrahiert, welche im Ergebnis homogene Merkmale aufweisen.

%\textbf{\acs{EEG}}
\paragraph{\acs{EEG}}
Auf jedes zu untersuchende \acs{EEG}-Signal wird zunächst eine Spektralanalyse mit einer Fensterung von 2 s und einer Überlappung von 1 s ausgeführt, um das Signal vom Zeit- in den Frequenzbereich zu überführen. Anschließend werden die charakteristischen Frequenzbänder (vgl. Kapitel \ref{psg}) extrahiert. Um die einzelnen Frequenzbänder möglichst eindeutig voneinander trennen zu können, werden gemäß Bashan et al. die Übergänge auf 0.5 Hz genau definiert (Tab. \ref{tab:frequenzen}). Es resultieren demnach für ein \acs{EEG}-Signal insgesamt fünf Zeitreihen mit einer Abtastfrequenz von 1 Hz. \parencite{bashan_network_2012}\\


\begin{table}[H] 
\centering
\begin{small}
\begin{tabular}{lc}
\toprule
\multicolumn{1}{l}{\textbf{Frequenzband}} & \multicolumn{1}{c}{\textbf{Frequenz in Hz}}\\  
\midrule
Delta-Wellen & 0.5 Hz - 3.5 Hz\\
Theta-Wellen & 4 Hz - 7.5 Hz\\
Alpha-Wellen & 8 Hz - 11.5 Hz\\
Sigma-Wellen & 12 Hz - 15.5 Hz\\
Beta-Wellen & 16 Hz - 19,5 Hz\\
\bottomrule
\end{tabular}
\caption[Frequenzbänder des \acs{EEG}]{Frequenzbänder des \acs{EEG} gemäß \parencite{bashan_network_2012}}
\label{tab:frequenzen}
\end{small}
\end{table}

\paragraph{\acs{EOG} und \acs{EMG}}
Aus \acs{EOG}- und \acs{EMG}-Signalen wird innerhalb von 2 s langen Fenstern mit einer Überlappung von 1 s die Varianz ermittelt und als Zeitreihe gespeichert.\parencite{bashan_network_2012}

\paragraph{\acs{EKG} und Atmung}
Zu den Atemsignalen zählen nasale Atmung sowie die Atemanstrengung in Brust und Bauch. Zur Extraktion homogener Zeitreihen aus \acs{EKG} und Atmung werden Herz- und Atemfrequenz berechnet und auf 1 Hz neu gesampelt. \parencite{bashan_network_2012}\\

Sämtliche extrahierte Zeitreihen weisen demnach eine identische Auflösung von 1 Hz mit gleicher Länge $N$ auf. Bei der Untersuchung der Korrelation zweier Zeitreihen $x$ und $y$ werden diese anschließend in $N_L$ sich überlappende Segmente $v$ eingeteilt. Jedes Segment $v$ weist eine Länge von 60 s bei einer Überlappung von 30 s auf (Formel~\ref{eq:zeitreihen}). Sodann erfolgt eine Normalisierung der Zeitreihen, so dass der Mittelwert 0 und die Standardabweichung 1 betragen. Auf diese Weise sind sämtliche Merkmale der ursprünglichen Signale homogenisiert und zwecks Ermittlung von Korrelationen vorverarbeitet.\parencite{bashan_network_2012}

\begin{equation}
L = 60 s \nonumber
\end{equation}
\begin{equation}
overlap = \frac{L}{2} \nonumber
\end{equation}
\begin{equation}
N_L = (2\frac{N}{L})-1
	\label{eq:zeitreihen}
\end{equation}

\subsection{Ermittlung von Korrelationen}\label{korrelationen}

Über zwei Zeitreihen $x$ und $y$ wird die Kreuzkorrelation durchgeführt, indem die eine Zeitreihe $x$ segmentweise und innerhalb jedes Segments $v$ sekundenweise über die zweite Zeitreihe $y$ geschoben wird. Für jeden Zeitpunkt der Verschiebung $T$ wird eine Zwischensumme der Kreuzkorrelation berechnet, wobei die entsprechenden Werte beider Zeitreihen miteinander multipliziert und anschließend alle Produkte addiert werden. Demnach entsteht für jeden Zeitpunkt der Verschiebung $T$ ein Eintrag in den Ergebnisvektor $xcorr$: \parencite{bashan_network_2012}

\begin{equation}
xcorr(x_v, y_v)(T) = \sum \limits_{i=1}^L x_v(i) \cdot y_v(T+i).
	\label{eq:xcorr}
\end{equation}

Der maximale Absolutwert innerhalb des Vektors $xcorr$ stellt in seinem Wert die höchstmögliche Ähnlichkeit der Zeitreihen und in seiner Position die Verschiebung $T$ zum Zeitpunkt dieser maximalen Korrelation dar. Für jedes Segment $v$ wird sodann die Verschiebung $T_0$ (Time Delay) zum maximalen Absolutwert aus dem Kreuzkorrelationsvektor $xcorr$ bestimmt. Als stabile Verbindungen gelten mindestens vier zusammenhängende Segmente $v$ (5 $*$ 30 s), deren Verschiebung $T_0$ annähernd gleich ($\pm$ 1~s) bleibt (Abb. \ref{fig:TimeDelay}). Je länger diese zusammenhängenden Epochen andauern, desto stabiler ist die Verbindung zwischen den beiden untersuchten Systemen. Anschließend wird der Prozentsatz der TDS im Verhältnis zur gesamten Zeitreihe berechnet. Je höher dieser Prozentanteil ausfällt, desto größer ist die Verbindungsstärke. Hierbei gilt ein Schwellenwert (Significance Threshold) von 7 \%, ab dem zwei Systeme tatsächlich als verbunden gelten. \parencite{bashan_network_2012}

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.45\textwidth]{img/Time_Delay.png}
	\caption[Zeitverschiebung (Time Delay) und stabile Verbindungen im \acs{TDS}-Verfahren]{Zeitverschiebung (Time Delay) und stabile Verbindungen im \acs{TDS}-Verfahren innerhalb eines physiologischen Netzwerks (unter Verwendung von \parencite{bashan_network_2012}):\\jeder Punkt steht für den Time Delay $T$ einer 30 s Epoche; fünf zusammenhängende 30 s Epochen mit stabilem Time Delay $T$ entsprechen vier zusammenhängenden Segmenten $v$;\\gelbe Punkte = stabile Verbindung zwischen Kinn-\acs{EMG} und Alpha-Wellen des \acs{EEG};\\rote Punkte = stabile Verbindung zwischen \acs{EKG} und \acs{EOG}; blaue Punkte = keine stabile Verbindung}
	\label{fig:TimeDelay}
\end{figure}

Auf diese Weise werden sämtliche Zeitreihen des Netzwerks miteinander verglichen. Im Ergebnis der \acs{TDS}-Analyse entsteht für jedes untersuchte Schlafstadium eine quadratische Matrix, welche die prozentualen Verbindungsstärken zwischen den Systemen farbig abbildet. Beispielhaft sind die Ergebnismatrizen für gesunde Probanden aus der Untersuchung von Krefting et al. \parencite{krefting_altersabhangigkeit_2016} (388 \acs{PSG}s aus der SIESTA-Studie) für jedes untersuchte Schlafstadium in der Abb. \ref{fig:TDS_Matrizen} dargestellt. Deutlich erkennbar ist hierbei die Ähnlichkeit zwischen Wachzustand und Leichtschlaf sowie zwischen Tief- und \acs{REM}-Schlaf. In den Zeilen und Spalten der Matrizen sind sämtliche untersuchten Systeme in gleicher Reihenfolge abgetragen (Herz, nasale Atmung, Atemanstrengung in Brust und Bauch, Kinn- und Beinbewegung, Augen sowie die jeweils fünf Frequenzbänder der sechs \acs{EEG}s). Dementsprechend stellt das obere rechte Dreieck die Spiegelung des unteren linken Dreiecks dar. Die Diagonale bildet die Autokorrelation der Systeme ab.\\

Das Verfahren der TDS stellt demnach ein Werkzeug zur Untersuchung der physiologischen Zusammenhänge im Schlaf dar. Hierbei ist jedoch nicht der Wert des Kreuzkorrelationskoeffizienten ausschlaggebend, sondern die Dauer der Korrelation. Auf diese Weise können nicht nur zeitgleiche, sondern auch zeitversetzte Ähnlichkeiten zweier Signale erkannt werden. \parencite{bashan_network_2012}

\begin{figure}[H]
	\centering
	\includegraphics[width = \textwidth]{img/TDS_W_LS_DS_REM.png}
	\caption[Ergebnis-Matrizen der \acs{TDS}]{Gemittelte Ergebnis-Matrizen der \acs{TDS}-Analyse für den Wachzustand, Leichtschlaf (oben), Tiefschlaf und \acs{REM}-Schlaf (unten) über die Kontrollgruppe der SIESTA-Studie (388 \acs{PSG}s):\\Verbindungsstärken sämtlicher Signalkopplungen in Prozent entsprechend der Colorbar; die abfallende Diagonale stellt die Autokorrelation der Systeme dar;\\$HR$ = Herz, $BR_{air}$ = nasaler Atemfluss, $BR_c$ = Atemanstrengung in der Brust, $BR_a$ = Atemanstrengung im Bauch, $Chin$ = Kinnbewegung, $Leg$ = Beinbewegung, $Eye1$ und $Eye2$ = Augen, $\delta$, $\theta$, $\alpha$, $\sigma$, $\beta$ = Frequenzbänder der frontopolaren ($Fp1$, $Fp2$), zentralen ($C3$, $C4$) und okzipitalen ($O1$, $O2$) Elektroden}
	\label{fig:TDS_Matrizen}
\end{figure}

\newpage

\section{Mittelwertberechnung}\label{calcmean}

In dieser Arbeit werden verschiedene Methoden der Mittelwertberechnung verwendet. Zur Mittelung der Verbindungsstärken innerhalb der Ergebnis-Matrizen aller Patienten, werden die Ergebnis-Matrizen der Patienten für jedes Schlafstadium übereinandergelegt, addiert und anschließend durch die Summe der Patienten dividiert gemäß

\begin{equation}
smean(x,y,s) = \frac{\sum \limits_{i=1}^N l_{tds}(x,y,s)_i}{N},
	\label{eq:smean}
\end{equation}

wobei $l_{tds}$ die prozentualen Verbindungsstärken beschreibt, $x$ und $y$ die Zeitreihen, $s$ das jeweilige Schlafstadium und $N$ die Anzahl der Patienten. Das Ergebnis \textit{smean} stellt demnach wiederum eine Matrix mit den gleichen Dimensionen wie die einzelnen Ergebnis-Matrizen der Patienten pro Schlafstadium dar. Bashan et al. definieren die Mittelwertberechnung über die Ergebnis-Matrizen der Patienten durch Aneinanderreihung der Zeitreihen und Division durch die Anzahl der Epochen des entsprechenden Schlafstadiums als 

\begin{equation}
gmean(x,y,s) = \frac{\sum \limits_{i=1}^N tds_{x,y}(v(s))_i}{\sum \limits_{i=1}^N v_{si}}
	\label{eq:gmean}
\end{equation}

mit den Zeitreihen $x$ bzw. $y$, der Anzahl der Patienten $N$, der Anzahl der Epochen pro Schlafstadium $v_s$ sowie den Schlafstadien $s$. Ein Vergleich beider Verfahren durch Krefting et al. zeigt keine signifikanten Unterschiede in den Ergebnissen beider Verfahren, so dass in dieser Arbeit lediglich die Mittelwertberechnung von \textit{smean} verwendet wird. \parencite{bashan_network_2012, krefting_age_2017}\\

Darüber hinaus wird zur Berechnung des globalen Mittelwerts die mittlere Verbindungsstärke über eine \acs{TDS}-Matrix berechnet gemäß

\begin{equation}
nmean(i,s) = \frac{\sum \limits_{x=1}^{N_n} \sum \limits_{y=x+1}^{N_n} l_{tds}(x,y,s)_i }{\frac{N_{n}^2 - N_n}{2}}
	\label{eq:nmean}
\end{equation}

mit den Zeilen $x$ und Spalten $y$ der Ergebnis-Matrix, der jeweiligen prozentualen Verbindungsstärke zweier Systeme $l_{tds}$ sowie $N_n$ als Anzahl der Netzwerkknoten bzw. Systeme. \textit{nmean} beschreibt demnach die Gesamtverbindungsstärke eines Patienten pro Schlafstadium, wobei diese über das obere rechte Dreieck und ohne die Autokorrelation der Systeme berechnet wird. \parencite{krefting_age_2017}

\section{Statistisches Matching}

Das statistische Matching ist eine Methode zum Auffinden geeigneter Datenpaare, sogenannte statistische Zwillinge, um Aussagen über die Abhängigkeit von Variablen zu treffen. Als Empfängergruppe gelten hierbei die Daten, zu denen passende statistische Zwillinge gefunden werden sollen. Diese Zwillinge sind einer Spendergruppe zu entnehmen und sollten den dazugehörigen Daten aus der Empfängergruppe in möglichst vielen Merkmalen ähneln, um deren Einfluss auf die abhängigen Variablen zu reduzieren. Durch Gegenüberstellung der Empfängerdaten mit deren statistischen Zwillingen aus der Spendergruppe können sodann abhängige Variablen untersucht werden. Das Vorgehen bei dem statistischen Matching besteht in der Auswahl geeigneter Merkmale sowie der Auswahl eines geeigneten Suchverfahrens. Bei der Auswahl der Merkmale gilt, dass entweder Schlüsselvariablen gewählt werden, welche einen signifikanten Einfluss auf die abhängige Variable ausüben, oder, da Schlüsselvariablen in der Praxis meist schwer zu identifizieren sind, all diejenigen Merkmale gewählen werden, welche einen Einfluss auf die abhängige Variable haben können. \parencite{weis_basiswissen_2005, bacher_statistisches_2002}\\

In dieser Arbeit wird das Verfahren \textit{random order, nearest available pair-matching method} von Smith zur Suche nach geeigneten Zwillingen verwendet. Hierbei stellen Empfänger- und Spendergruppe zwei zufällig sortierte Datensätze dar. Für den ersten Datensatz der Empfängergruppe wird beginnend mit dem ersten Datensatz innerhalb der Spendergruppe ein passender Zwilling gesucht. Hierbei erfolgt ein Abgleich, ob der entsprechende Datensatz der Spendergruppe ähnliche Merkmale aufweist wie der aktuelle Datensatz der Empfängergruppe. Ist ein Zwilling gefunden, so wird dieser für weitere Matchings gestrichen. Werden mehrere statistische Zwillinge gefunden, so wird der erste ausgewählt. Anschließend wird das Verfahren mit dem zweiten bis n-ten Datensatz der Empfängergruppe in gleicher Weise wiederholt, bis die Empfängergruppe vollständig durchlaufen ist. \parencite{smith_matching_1997, bacher_statistisches_2002}

\section{Statistische Hypothesen und Prüfgrößen}

Statistische Hypothesen stellen Behauptungen über bestimmte Eigenschaften von Variablen dar. In der Regel sind solche Hypothesen schwer direkt zu überprüfen. Beispiele können die Hypothese zwar untermauern, jedoch nicht beweisen. Ein Gegenbeispiel genügt, um die Hypothese zu widerlegen. Aus diesem Grund werden in der Statistik Gegenhypothesen (Nullhypothesen) aufgestellt. Gelingt es, diese Nullhypothese zu widerlegen, kann indirekt die vorherige Behauptung bestätigt werden. Um die Nullhypothese abzulehnen oder sie zu bestätigen, werden anhand von unabhängig gewonnenen Stichproben statistische Tests durchgeführt. Bei vielen solcher Tests werden Teststatistiken berechnet. Diese stellen Prüfgrößen dar, anhand derer entschieden werden kann, ob die Nullhypothese abzulehnen oder zu bestätigen ist. Hierzu dient der P-Wert, welcher das kleinste Signifikanzniveau $\alpha$ darstellt, wonach die Nullhypothese abgelehnt werden kann. In der Regel liegt das Signifikanzniveau bei 0,05. Liegt der p-Wert unterhalb dieses Signifikanzniveaus, so kann anhand der untersuchten Stichproben mit einer Wahrscheinlichkeit von mindestens 95~\% die Nullhypothese abgelehnt und der Alternativhypothese (der anfänglichen Behauptung) zugestimmt werden. Liegt der errechnete p-Wert oberhalb des Signifikanzniveaus, so kann die Nullhypothese nicht abgelehnt werden und die anfängliche Behauptung gilt als widerlegt. Der p-Wert gilt demnach als Maß für die Evidenz der Nullhypothese. Statistische Tests, welche mit p-Werten arbeiten, werden als Signifikanztests bezeichnet. Untersuchungen statistischer Hypothesen führen demnach dazu, dass es sich lediglich um Wahrscheinlichkeitsaussagen mit statistischer Signifikanz handelt. Der p-Wert gilt dabei stets lediglich für die gewählte Stichprobe und kann nicht zwingend verallgemeinert werden. \parencite{hedderich_angewandte_2016}

\section{Normalverteilung und Varianz}

Viele statistische Verfahren setzen eine Normalverteilung der Stichproben voraus. Um diese Voraussetzung zu prüfen, wird der Shapiro-Wilk-Test verwendet. Hierbei wird die Nullhypothese aufgestellt, dass eine Stichprobe~$x$ normalverteilt ist. Insbesondere bei kleinen Stichproben mit weniger als 50 Messwerten zeichnet sich der Shapiro-Wilk-Test durch eine hohe Teststärke aus. Auf Grundlage geschätzter Varianzen wird mit dem Shapiro-Wilk-Test für die gewählte Stichprobe die Teststatistik

\begin{equation}
W =  \frac{b^2}{(n - 1)s^2}
	\label{eq:shapiro}
\end{equation}

berechnet. Die Teststatistik $W$ stellt dabei eine Kennzahl dar, welche das Verhältnis zwischen geschätzter ($b^2$) und tatsächlicher Varianz ($s^2$) der Stichprobe angibt. Zunächst werden die Stichprobenwerte aufsteigend sortiert, so dass $x_1 < x_2 < ... < x_n$ gilt. Anschließend wird die geschätzte Varianz berechnet durch

\begin{equation}
b =  \sum \limits_{i=1}^k \alpha_{n-i+1} \cdot (x_{n-i+1} - x_i), \text{ mit} \begin{cases}
     k = \frac{n}{2} & \text{für gerades n}\\
     k = \frac{n-1}{2} & \text{für ungerades n}
   \end{cases}
	\label{eq:var1}
\end{equation}

und die tatsächliche Stichprobenvarianz durch 

\begin{equation}
s^2 = \frac{\sum \limits_{i=1}^n (x_i - \overline{x})^2}{n - 1}.
	\label{eq:var1}
\end{equation}

Hierbei entspricht $n$ der Menge der Stichprobenwerte und $\alpha$ einem Signifikanzniveau, welches in dieser Arbeit auf 0,05 (5~\%) festgelegt wird. Die Teststatistik $W$ wird anschließend mit einem kritischen Wert $W_{kritisch}$ verglichen, welcher der Shapiro-Wilk-Tabelle entnommen werden kann. Ist $W > W_{kritisch}$, wird die Null-Hypothese bestätigt und eine Normalverteilung der Grundgesamtheit wird angenommen. Darüber hinaus gibt der p-Wert die Wahrscheinlichkeit an, dass die Stichprobe aus einer normalverteilten Grundgesamtheit stammt. Ist der p-Wert größer als das Signifikanzniveau $\alpha$, so kann die Nullhypothese bestätigt werden. \parencite{frank_einfach_2006, shapiro_analysis_1965}\\

An eine normalverteilte Grundgesamtheit ist eine ähnliche Varianz der beiden zu untersuchenden unabhängigen Zufallsstichproben gebunden. Um die Varianzen beider Stichproben zu vergleichen, wird die Nullhypothese aufgestellt, dass die Varianzen homogen sind, die Alternativhypothese lautet entsprechend umgekehrt:

\begin{equation}
H_0: \sigma_{1}^2 = \sigma_{2}^2\nonumber
\end{equation}
\begin{equation}
H_A: \sigma_{1}^2 \neq \sigma_{2}^2.
	\label{eq:Ho_varianz}
\end{equation}

Die Varianz gibt die mittlere quadratische Abweichung um den Mittelwert an und wird berechnet durch

\begin{equation}
\sigma^2 = \frac{1}{n-1}\sum \limits_{i=1}^n |x_i - \overline{x}|^2
\end{equation}

mit der Stichprobe $x$, der Anzahl der Werte der Stichprobe $n$ sowie dem Mittelwert dieser Werte $\overline{x}$. Bei kleinem bis mittelgroßem Stichprobenumfang können mit Hilfe des F-Tests die Varianzen der beiden Stichproben auf Homogenität überprüft werden. Hierzu werden die Prüfsumme anhand des Quotienten der Varianzen

\begin{equation}
\hat{F} = \frac{\sigma_{1}^2}{\sigma_{2}^2}
\end{equation}

sowie der p-Wert berechnet. Anschließend erfolgt ein Abgleich mit den entsprechend tabellierten F-Werten. Ist $\hat{F}\leq F$, so ist die Nullhypothese zu bestätigen. Liegt der p-Wert oberhalb des Signifikanzniveaus, so gilt dasselbe und die Alternativhypothese ist abzulehnen. \parencite{hedderich_angewandte_2016}

\section{Vergleich von Populationen}

Um die Population der Insomniepatienten mit der Population der statistischen Matches zu vergleichen, wird ein zweiseitiger Student's T-Test verwendet. Voraussetzung für die Verwendung dieses Verfahrens ist eine Normalverteilung beider Stichproben sowie die Homogenität der Varianzen. Sind diese Voraussetzungen gegeben, so wird mit Hilfe des T-Tests ein Mittelwertvergleich durchgeführt. Als Nullhypothese wird zunächst definiert, dass die Mittelwerte der beiden Stichproben gleich sind und diese demnach aus der gleichen Grundgesamtheit stammen. Anschließend wird die T-Statistik anhand der Mittelwerte und Varianzen der Stichproben berechnet durch

\begin{equation}
T = \frac{\overline{x_i} - \overline{y_i}}{\sqrt{\frac{s_{x}^2}{n_x}} + \frac{s_{y}^2}{n_y}}
\end{equation}

mit den beiden Stichproben $x$ und $y$, dem Stichprobenumfang $n$, den Mittelwerten $\overline{x_i}$ bzw. $\overline{y_i}$ sowie den Varianzen $s_{x}^2$ und $s_{y}^2$. Anschließend wird die T-Statistik mit einem tabellierten  Wert verglichen. Ist $T$ größer, so unterscheiden sich die Mittelwerte und die Nullhypothese kann abgelehnt werden. \parencite{frank_einfach_2006, hedderich_angewandte_2016}\\

Bei Vorliegen einer Normalverteilung, jedoch heterogenen Varianzen wird der Welch-Test verwendet. Die T-Statistik wird bei gleichem Stichprobenumfang identisch gebildet. Lediglich die Berechnung der Freiheitsgrade unterscheidet sich vom T-Test. Anhand der Freiheitsgrade wird der entsprechende Wert in der Tabelle abgelesen. Freiheitsgrade werden auf Grundlage der Anzahl der Stichprobenwerte berechnet. Bei dem Welch-Test erfolgt die Berechnung gemäß 

\begin{equation}
FG = n - 1,
\end{equation}

bei dem T-Test mit gleichen Varianzen hingegen gemäß

\begin{equation}
FG = 2n - 2.
\end{equation}

Da in die Stichprobenumfänge in dieser Arbeit gleich sind, wird auf die Freiheitsgrade nicht näher eingegangen. \parencite{hedderich_angewandte_2016, cressie_how_1986}\\

Handelt es sich bei den Stichproben nicht um normalverteilte Daten, so muss der nichtparametrische Wilcoxon-Rangsummentest (U-Test) angewendet werden. Dieser Test setzt keinerlei Anforderungen an die Verteilung voraus (verteilungsunabhängig), sofern anzunehmen ist, dass die Stichproben aus der gleichen Grundgesamtheit stammen, und basiert auf Rangdaten. Zunächst wird demnach die Nullhypothese aufgestellt, dass die Stichproben aus der gleichen Grundgesamtheit stammen. Bei dem U-Test werden sämtliche Werte beider Stichproben ($N = n_x + n_y$) der Größe nach sortiert und ihnen 1 bis $N$ Ränge zugeordnet. Zusätzlich wird vermerkt, welcher Stichprobe jeder Rang angehört. Anschließend werden die Rangzahlen für jede Stichprobe summiert. Sofern in der einen Stichprobe eher kleinere Werte als  in der anderen Stichprobe enthalten sind, so werden sich die summierten Rangzahlen unterscheiden. Die Prüfgrößen $U_1 und U_2$ werden anhand dieser summierten Rangzahlen für beide Stichproben berechnet gemäß

\begin{equation}
U_1 = n_{x}n_{y} + \frac{n_{x}(n_{x}+1)}{2} - R_x \nonumber
\end{equation}
\begin{equation}
U_2 = n_{x}n_{y} + \frac{n_{y}(n_{y}+1)}{2} - R_y
\end{equation}

mit der Anzahl der Werte $x$ und $y$ in den beiden Stichproben und den summierten Rangzahlen $R_x$ und $R_y$ für beide Stichproben. Die kleinere der beiden Prüfgrößen stellt die gesuchte Prüfgröße dar. Diese wird sodann anhand der Freiheitsgrade mit dem entsprechend tabellierten kritischen Wert verglichen. Sofern $U\leq U_{kritisch}$ ist, so kann die Nullhypothese verworfen werden. Bei dem U-Test kann es vorkommen, dass gleiche Ränge vergeben werden, sogenannte Bindungen. Hierbei wird die mittlere Rangzahl anhand der summierten Ränge gleicher Werte dividiert durch die Anzahl der gleichen Werte. In solchen Fällen kann der p-Wert nicht exakt bestimmt werden. \parencite{hedderich_angewandte_2016}\\

Bei dem T-Test, dem Welch-Test sowie dem U-Test müssen die p-Werte unterhalb des Signifikanzniveaus liegen, um die Nullhypothese zu verwerfen. Dies bedeutet wiederum, dass der Alternativhypothese, dass sich die Populationen signifikant unterschieden, mit einer Wahrscheinlichkeit von mindestens 95~\% zugestimmt werden kann. \parencite{hedderich_angewandte_2016}\\

\section{Multiples Testproblem}

Da es sich lediglich um Wahrscheinlichkeitsaussagen handelt, kann es bei Hypothesentests durch eine fälschliche Ablehnung der Nullhypothese (flasch-positive Resultate) zu dem sogenannten $\alpha$-Fehler (oder Fehler 1. Art) kommen. Um diesen Fehler zu kontrollieren, sollten möglichst wenige Hypothesentests durchgeführt werden. Ist die Anwendung mehrerer Hypothesentests jedoch zwingend erforderlich, beispielsweise durch Prüfung verschiedener abhängiger Variablen zweier Stichproben, kumuliert sich der berechnete Fehler $p$ mit jedem Test gemäß

\begin{equation}
p = 1-0,95^n
\end{equation}

mit der Anzahl der Hypothesentests $n$ sowie einem Signifikanzniveau von 5~\%. Bei 20 Tests ergäbe der globale Fehler bei gleichem Signifikanzniveau bereits $1-0,95^20=0,64$ und die Wahrscheinlichkeit einer fehlerhaften Aussage läge demnach bei 64~\%. Aus diesem Grund werden bei multiplen Tests Korrekturverfahren eingesetzt. In dieser Arbeit wird das Bonferroni-Verfahren verwendet, welches als das konservativste Korrekturverfahren gilt. Hierbei wird jeder p-Wert $p_i$ der einzelnen Hypothesentests mit der Anzahl der durchgeführten Hypothesentests $n$ multipliziert gemäß

\begin{equation}
p_{i}^{*} = p_i \cdot n.
\end{equation}

Anschließend müssen die p-Werte $p_{i}^{*}$ jedes Tests mit dem gewählten Signifikanzniveau $\alpha$ verglichen werden. Solche p-Werte, die kleiner als $\alpha$ sind, gelten sodann als signifikant. Auf diese Weise können viele falsch-positive Resultate ausgeschlossen werden. \parencite{hedderich_angewandte_2016}

\section{Lineare Regression}

Das Ziel der linearen Regression ist die Vorhersage einer Zielgröße anhand bekannter Variablen. Diese Vorhersage beruht zum einen auf unabhängigen Werten $x_i$ und zum anderen auf von $x_i$ abhängigen Werten $y_i$. $x$ und $y$ stellen demnach Messreihen dar, wobei $y$ von $x$ abhängt. Ein einfaches Beispiel stellt die Schuhgröße in Abhängigkeit von der Körpergröße dar. Die Wertpaare lassen sich als Punktwolke grafisch darstellen. Die lineare Regression stellt hierbei ein statistisches Werkzeug zur Bestimmung einer idealen Regressionsgeraden durch die Punktwolke gemäß

\begin{equation}
y = a + b \cdot x + \epsilon
	\label{eq:linreg}
\end{equation}

dar mit $y$ als abhängige Zielgröße, der unabhängigen Variablen $x$, der Konstanten $a$ als Schnittpunkt mit der y-Achse, dem Regressionskoeffizienten $b$ sowie einem Fehlerwert $\epsilon$. Der Regressionskoeffizient wird anhand der einzelnen Werte in den Messreihen sowie deren Mittelwerte gemäß

\begin{equation}
b = \frac{\sum \limits_{i=1}^n(x_i-\overline{x}) \cdot (y_i-\overline{y})}{\sum \limits_{i=1}^n(x_i-\overline{x})^2} 
	\label{eq:regcoef}
\end{equation}

berechnet, wobei $\overline{x}$ und $\overline{y}$ die jeweiligen Mittelwerte der Messreihen darstellen. Die Konstante $a$ wird anschließend anhand von 

\begin{equation}
a = \overline{y} - b \cdot \overline{x}
	\label{eq:konstante}
\end{equation}

berechnet. Mit Hilfe von $a$ und $b$ kann demnach für jedes $x_i$ eine Vorhersage für den Zielwert $y_i$ getroffen werden. Da der Zielwert meist Schwankungen unterliegt, definiert $\epsilon$ einen Fehlerwert. \parencite{frank_einfach_2006}\\

Mit Hilfe eines linearen Modells wird versucht, die Beziehung zwischen den Variablen $x_i$ und $y_i$ zu beschreiben. Zu diesem Zweck werden zunächst eine Null-Hypothese sowie eine Alternativ-Hypothese aufgestellt.

\begin{equation}
H_0: \text{y hängt nicht von x ab}. \nonumber
\end{equation}
\begin{equation}
H_A: \text{y hängt von x ab}
	\label{eq:null-hypo}
\end{equation}

Liegt der p-Wert unterhalb des Signifikanzniveaus von 0,05, kann die Null-Hypothese abgelehnt werden, so dass mit hoher Wahrscheinlichkeit (mindestens 95~\%) ein linearer Zusammenhang zwischen den Variablen besteht. Liegt der p-Wert oberhalb des Signifikanzniveaus, so wird der Null-Hypothese zugestimmt und es existiert kein linearer Zusammenhang zwischen den Variablen. \parencite{frank_einfach_2006}

\section{Korrelationsverfahren}

Korrelationsverfahren dienen zur Messung des Zusammenhangs zweier Variablen. Mit Hilfe des Korrelationskoeffizienten nach Pearson wird der lineare Zusammenhang zweier Messreihen $x$ und $y$ ermittelt durch

\begin{equation}
r =  \frac{\sum \limits_{i=1}^n (x_i-\overline{x}) \cdot (y_i-\overline{y})}{\sqrt{\sum \limits_{i=1}^n (x_i-\overline{x})^2} \cdot \sqrt{\sum \limits_{i=1}^n (y_i-\overline{y})^2}}.
	\label{eq:pearson}
\end{equation}

Der Korrelationskoeffizient nach Spearman, auch Rangkorrelation genannt, wird nicht anhand der Messwerte, sondern anhand ihrer Ränge berechnet. Hierzu wird für jeden Wert beider Messreihen ein Rang vergeben in der Form, dass beispielsweise die höchste Körpergröße den Rang 1 erhält und die geringste den letzten Rang innerhalb der Messreihe. Für die Messreihe der Schuhgröße erfolgt die Rangvergabe nach identischem Muster, so dass die größte Schuhgröße den Rang 1 und die kleinste den letzten Rang der Messreihe erhält. Bei der grafischen Darstellung dieser Ränge geht der Zusammenhang der vorherigen Messwertpaare demnach verloren. Berechnet wird die Rangkorrelation in gleicher Weise wie der Korrelationskoeffizient nach Pearson, jedoch werden die Messwerte $x_i$ und $y_i$ durch ihre Ränge ersetzt, so dass 

\begin{equation}
rho =  \frac{\sum \limits_{i=1}^n (Rang(x_i)-\overline{Rang(x)}) \cdot (Rang(y_i)-\overline{Rang(y)})}{\sqrt{\sum \limits_{i=1}^n (Rang(x_i)-\overline{Rang(x)})^2} \cdot \sqrt{\sum \limits_{i=1}^n (Rang(y_i)-\overline{Rang(y)})^2}}
	\label{eq:spearman}
\end{equation}

gilt. Demnach wird mit Hilfe des Korrelationskoeffizienten nach Spearman nicht der lineare, sondern der monotone Zusammenhang zwischen zwei Variablen ermittelt. \parencite{frank_einfach_2006}\\

Für beide Verfahren gilt, dass der Korrelationskoeffizient $r$ bzw. $rho$ stets zwischen -1 und 1 liegt. Bei einem Korrelationskoeffizienten $\approx$ 0 besteht keine Korrelation. Sind $r$ bzw. $rho$ kleiner als der Wert 0, existiert eine negative Korrelation, so dass steigende $x$-Werte abfallende $y$-Werte bewirken. Bei einem Korrelationskoeffizienten, der größer als der Wert 0 ist, besteht hingegen eine positive Korrelation, so dass steigende $x$-Werte wachsende $y$-Werte hervorrufen. In einem linearen Modell wird für beide Verfahren die Null-Hypothese aufgestellt, dass die Variablen $x$ und $y$ nicht korrelieren, wobei der p-Wert wiederum die Signifikanz der Korrelation zwischen den Variablen repräsentiert. Ein p-Wert, der kleiner als das Signifikanzniveau von 0,05 ist, gilt als signifikante Korrelation, so dass die Null-Hypothese abgelehnt werden kann. \parencite{frank_einfach_2006}

\section{Matlab}

Matlab ist eine in den 1970er Jahren in Fortran entwickelte und von The Mathworks vertriebene kommerzielle Software. Sie dient der Analyse und Berechnung numerischer Daten und Problemstellungen sowie der Visualisierung der Ergebnisse. Berechnungen basieren in Matlab (von MATrix LABoratory) auf Matrizen. Aus diesem Grund ist die Nutzung von Matlab im mathematischen, technischen und wissenschaftlichen Kontext stark verbreitet. Insbesondere die Signalverarbeitung kann mit Matlab geeignet umgesetzt werden.\footnote{\url{https://de.mathworks.com/products/matlab.html} und \url{https://de.mathworks.com/solutions.html?s_tid=gn_sol} (Stand: 23.02.2017)}\\

Matlab ähnelt in seiner Syntax zwar der Programmiersprache C, stellt jedoch eine proprietäre Programmier- bzw. Skriptsprache dar. Anwendungen können entweder in Skripten geschrieben oder als atomare Funktionen entwickelt werden, was eine Erstellung der in Matlab charakteristischen Toolboxes unterstützt. Matlab ist darüber hinaus eine Sprache mit dynamischer Typisierung. Dies erhöht die Lesbarkeit des Codes und die Produktivität des Entwicklers. Funktionen sind hierdurch einerseits flexibler, da sie oft weniger Code enthalten als in einer stark typisierten Sprache. Andererseits müssen Funktionen nicht überladen werden, um für unterschiedliche Datentypen zu gelten. Variablen müssen darüber hinaus nicht mit expliziten Datentypen initialisiert werden. Als interpretierte Programmiersprache (oder Skriptsprache) bietet Matlab ferner den Vorteil, dass Code schnell getestet und angewendet werden kann - ohne den zeitaufwendigen Zwischenschritt der Kompilierung. Dies ermöglicht das Testen von Skripten oder Skriptausschnitten mit beliebigen Startpunkten der Ausführung sowie das Testen atomarer Funktionen, ohne eine Main-Klasse ausführen zu müssen, wie z. B. in Java. Eine Erstellung eigenständig lauffähiger Programme ist jedoch durch Erweiterung der Grundfunktionen von Matlab ebenfalls möglich. \parencite{stein_programmieren_2012}\\

Nachteil von Matlab ist zum einen die Speicherverwaltung, da der Standarddatentyp für numerische Variablen \texttt{double} mit einem Speicherplatz von 64 bit ist. Eine starke Typisierung könnte hingegen das Speichermanagement optimieren. Zum anderen stellt Matlab keine kompilierte Sprache dar, so dass in der Grundfunktionalität die Erzeugung lauffähiger Anwendungsprogramme nicht möglich ist. Darüber hinaus dient die statische Codeanalyse lediglich der Überprüfung des Datenflusses, es entfällt jedoch die Überprüfung des Programmcodes zur Übersetzungszeit, so dass Fehler erst bei Ausführung der Funktion erkannt werden.\\

In dieser Arbeit soll für die Anwendung der \acs{TDS}-Analyse Matlab verwendet werden, da die verwendeten Analyseverfahren mathematisch basiert sind und Matlab einerseits als funktionale Sprache für numerische Berechnungen besonders geeignet ist und andererseits Berechnungen einfacher als in kompilierten Sprachen zu implementieren sind. Da im Rahmen dieser Arbeit kein eigenständiges System entwickelt wird, sondern Analyseverfahren angewendet werden sollen, ist Matlab als interpretierte Sprache mit dynamischer Typisierung optimal geeignet, um schnell Zwischenergebnisse zur erzeugen und diese zu verifizieren, auszuwerten und grafisch darzustellen. Aufgrund der Aufgabenstellung sowie des relativ kleinen Untersuchungsdatensatzes (Kapitel \ref{analyse}) kann auch der Nachteil einer weniger effizienten Speicherverwaltung vernachlässigt werden. Da Matlab darüber hinaus unter anderem für die Signalverarbeitung ausgelegt ist, soll die Anwendung des \acs{TDS}-Verfahrens in Matlab umgesetzt werden. Hierfür wird  Matlab in der Version 2015b verwendet.

%benutzt, weil: Flexibilität, Erweiterbarkeit (Toolboxes von Mathworks), und Workspace möglich, numerische Stabilität (kein Datenverlust oder Rundungsfehler), Effizienz und Optimierung der Berechnungen (Beispiel rekursive Fakultät)
%
%überschreiben: bei Vererbung, Änderung des Methodenkörpers
%überladen: Änderung des Methodenheaders (Parameteranzahl oder -datentyp) = Methodensignatur


\section{R}

R ist als Teil des GNU Projekts eine Open-Source-Programmierumgebung zur Anwendung statistischer Verfahren sowie zur grafischen Darstellung von Daten und steht seit dem Jahr 1995 unter der GNU General Public License. Die Sprache R wurde im Jahr 1992 von Robert Gentleman und Ross Ihaka in C und Fortran sowie später in R selbst in Anlehnung an die Programmiersprache S entwickelt und stellt eine interpretierte Multiparadigmensprache mit dynamischer Typisierung dar. Dies bezieht sich darauf, dass R zum einen objektorientiert und zum anderen funktional ist. Sämtliche Daten (z. B. Variablen, Funktionen, Ausdrücke etc.) in R stellen Objekte dar, wohingegen sämtliche Aktionen Funktionsaufrufe repräsentieren. Eine R-Distribution enthält einige Standard-Pakete, welche Basisfunktionen für statistische Berechnungen und grafische Darstellungen realisieren, und kann simpel über das \acl{CRAN} (\acs{CRAN}) erweitert werden.\footnote{https://www.r-project.org/about.html (Stand: 18.03.2017)} Das Paket \texttt{R.matlab} beispielsweise ermöglicht das Lesen und Schreiben von Matlab-Code (.m-Dateien) und gespeicherten Variablen (.mat-Dateien) sowie das Starten von Matlab und die Ausführung von Matlab-Code im Hintergrund aus R heraus. Im \acs{CRAN} können Nutzer darüber hinaus selbst entwickelte Pakete zur Verfügung stellen. \parencite{adler_r_2012, manderscheid_sozialwissenschaftliche_2012}\\

Jedes Objekt in R besitzt einen bestimmten Datentypen und gehört einer Klasse an. Funktionen sind beispielsweise Objekte der Klasse \texttt{function}. R basiert auf Vektoren. Die grundlegenden Objekte in R stellen demnach Zeichen- und numerische Vektoren und darüber hinaus Matrizen, Arrays, Listen und Data Frames dar. Vektoren sind eindimensional, Matrizen zwei- und Arrays mehrdimensional. Listen haben die Eigenschaft, unterschiedlich lange Objekte zu speichern. Data Frames können hingegen als Tabellen verstanden werden. Der Standard-Datentyp in R ist wie in Matlab \texttt{double}. Es können ebenso Funktionen und Skripte erstellt werden. Darüber hinaus kann in R das Paket \texttt{R.compiler} zur Erstellung lauffähiger Programme geladen werden. \parencite{adler_r_2012, manderscheid_sozialwissenschaftliche_2012}\\

Als interpretierte Sprache mit dynamischer Typisierung bietet R die gleichen Vorteile wie Matlab. Ein Nachteil liegt darin, dass R die Daten vor der Weiterverarbeitung zunächst in den RAM lädt. Dies kann bei besonders großen Datensätzen zu Laufzeitproblemen führen. Gegenüber Matlab bietet R jedoch den Vorteil, dass es speziell für statistische Berechnungen ausgelegt ist mit einer besonderen Stärke in der grafischen Darstellung von Daten. In Matlab hingegen sind in der Grundausführung lediglich statistische Standardfunktionen implementiert, welche durch eine Statistik-Toolbox erweitert werden können. R ist vor allem im wissenschaftlichen Umfeld weit verbreitet. Dies zeigt das IEEE-Ranking\footnote{http://spectrum.ieee.org/static/interactive-the-top-programming-languages-2016 (Stand: 19.03.2017)} über die Top-Prgrammiersprachen im Jahr 2016, in dem R mit einem Ranking von 87,9 (von 100) Platz 5 belegt, während Matlab mit einem Ranking von 68,5 lediglich Platz 10 erreicht. Aus diesen Gründen wird für die statistische Analyse der \acs{TDS}-Ergebnisse R verwendet. Darüber hinaus wird R Studio als Open-Source-IDE für R genutzt, da es zum einen eine umfangreichere grafische Oberfläche von R implementiert mit R-Konsole, Workspace, Editor und History, während R selbst lediglich die Konsole umfasst. Zum anderen bietet R Studio u. a. Syntax-Highlighting, Ausführung aus dem Editor heraus sowie Debugging-Funktionen.\footnote{https://www.rstudio.com/products/RStudio/ (Stand: 18.03.2017)} In dieser Arbeit wird R in der Version 3.3.2 und R Studio in der Version 1.0.136 verwendet.

\section{Vorgehen}

Um das \acs{TDS}-Verfahren auf die hiesigen Daten der Insomniepatienten anwenden zu können, muss im ersten Schritt eine Datenanalyse erfolgen. Insbesondere die Heterogenität der Voraussetzungen der Patienten durch verschiedene Begleiterkrankungen und ggf. unterschiedliche Arten der Insomnie mit differenter Ausprägung der Schlafstadien macht Überlegungen notwendig, inwiefern eine Vergleichbarkeit der Daten untereinander sowie mit den Ergebnissen von Krefting et al. \parencite{krefting_age_2017} herbeigeführt werden kann. Aus diesem Grund werden Kriterien definiert, anhand derer ggf. Gruppen mit ähnlichen Merkmalen gebildet und Daten bei Nichterfüllung dieser Kriterien von den weiteren Untersuchungen ausgeschlossen werden müssen. Im nächsten Schritt werden die Matlab-Funktionen des \acs{TDS}-Pakets um Codeblöcke zur Anwendung auf den hiesigen Datensatz der Charit\'{e} Berlin erweitert. Dies ist notwendig, da sich der hiesige Datensatz in Anzahl und Sortierung der Signale sowie in den Hypnogrammen von dem von Krefting et al. untersuchten Datensatz unterscheidet. Anschließend wird das \acs{TDS}-Verfahren auf Daten der jeweiligen Gruppen ausgeführt. Um Gruppenergebnisse der Datenanalyse zusammenzufassen und auszuwerten, werden die Mittelwerte \textit{smean} mit Matlab berechnet und die Ergebnisse gegenübergestellt sowie mit den gemittelten \acs{TDS}-Matrizen der SIESTA-Daten verglichen.\\

Um Aussagen über die Ausprägung der Verbindungsstärken bei Insomiepatienten treffen zu können, werden die gemittelten Ergebnis-Matrizen der Insomniepatienten untereinander in unterschiedlichen Gruppen (weibliche gegenüber männlichen, jüngere gegenüber älteren Insomniepatienten etc.) sowie mit den Ergebnissen gesunder Probanden durch Subtraktion der Matrizen in Matlab verglichen. Darüber hinaus werden die (Sub-)Netzwerke gemäß Krefting et al. für das gesamte \acs{TDS}-Netzwerk, für das Subnetz des Gehirns sowie für dies Subnetze von Gehirn und Peripherie sowie der Peripherie untereinander mit Matlab gebildet und für diese die globalen Verbindungsstärken \textit{nmean} und die Anzahl der Verbindungen gegenübergestellt.\\

Im nächsten Schritt werden mit Hilfe von R statistische Verfahren angewendet, um die Ergebnisse mit denen von Krefting et al. bei gesunden Probanden zu vergleichen. Zur Untersuchung der Altersabhängigkeit erfolgt eine Gegenüberstellung der globalen Verbindungsstärke \textit{nmean} und dem Alter jedes Insomniepatienten. Eine lineare Regression sowie die Anwendung der Korrelationsverfahren sollen Aufschluss über den Zusammenhang beider Parameter geben.\\

Darüber hinaus werden mit Hilfe des statistischen Matchings geeignete Zwillinge für die Insomniepatienten identifiziert. Als Spendergruppe dienen hierbei die Datensätze aus der SIESTA-Studie, welche ebenfalls von Krefting et al. verwendet wurden. Für die Zuordnung eines statistischen Zwillings werden Alter und Geschlecht als gemeinsame Merkmale für das Matching herangezogen. Eine Altersdifferenz von $\pm$1 Jahr wird als Schwellenwert definiert. Als abhängige Variablen gelten die einzelnen Verbindungsstärken innerhalb der \acs{TDS}-Matrix jedes Patienten/Probanden. Anhand des statistischen Matchings werden anschließend Signifikanztests zur Bestimmung der Ähnlichkeit von Insomniepatienten und gesunden Probanden in Bezug auf die Ausprägung der Verbindungsstärken in den \acs{TDS}-Matrizen durchgeführt. Als Stichproben gelten zum einen die Insomniepatienten und zum anderen die statistischen Matches. Zunächst erfolgt für beide Stichproben ein Shapiro-Wilk-Test zur Prüfung einer Normalverteilung. Sind beide Stichproben normalverteilt, wird die Homogenität der Varianzen mit Hilfe des F-Tests überprüft. Bei homogenen Varianzen erfolgt sodann der zweiseitige T-Test, bei heterogenen der zweiseitige Welch-Test. Liegt bei mindestens einer Stichprobe keine Normalverteilung der Daten vor, so kommt der U-Test zum Einsatz. Als Nullhypothese wird jeweils die Gleichheit der Populationen bzw. deren Mittelwerte formuliert. Das Signifikanzniveau wird in allen Verfahren gemäß Krefting et al. auf 0,05 festgelegt. Da als abhängige Variablen sämtliche Verbindungen innerhalb des \acs{TDS}-Netzwerks gelten, werden insgesamt $28^2-28/2 = 378$ Hypothesentests durchgeführt. Aus diesem Grund erfolgt im Anschluss die Bonferroni-Korrektur.\\

Anhand dieser Untersuchungen sollen \acs{TDS}-Merkmale für Insomniepatienten abgeleitet werden. Darüber hinaus werden Alters- und Geschlechtsabhängigkeiten des \acs{TDS}-Verfahrens bei Insomniepatienten beschrieben. Die Ergebnisse werden in Hinblick auf die Beschaffenheit der Daten sowie im Vergleich zu den Ergebnissen gesunder Probanden diskutiert. Sämtliche Plots von \acs{TDS}-Matrizen sowie der (Sub-) Netzwerke werden mit Matlab erstellt. Die übrigen Plots werden mit Hilfe von R erzeugt.

%Darüber hinaus wird ein Leitfaden für die Vorauswahl der Daten sowie für die Anwendung des \acs{TDS}-Verfahrens auf Datensätze der Charit\'{e} Berlin entwickelt. 

%da lediglich die Anwendung verschiedener Verfahren programmiert werden soll, jedoch kein Programm zur häufigen Anwendung, ist keine Kompilierung notwendig und Matlab vor allem aufgrund des schnellen Testens geeignet, um die hiesige Problemstellung zu lösen. 
%hauptsächlich werden Skripte (in dieser Arbeit zur einmaligen Ausführung) erstellt